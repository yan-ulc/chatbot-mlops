{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362a7eb2",
   "metadata": {},
   "source": [
    "# End to End Chatbot MLOPS\n",
    "\n",
    "## Overview\n",
    "\n",
    "## Archtecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a6941",
   "metadata": {},
   "source": [
    "### Repository Structure\n",
    "\n",
    "```\n",
    "/chatbot-mlops\n",
    "├──.github/workflows/   # GitHub Actions for CI/CD\n",
    "├── docs/                # Project documentation (README, model cards, etc.)\n",
    "├── infra/               # Infrastructure configs (Docker, K8s, Helm)\n",
    "│   ├── mlflow/\n",
    "│   ├── monitoring/\n",
    "│   └── postgres/\n",
    "├── labeling/            # Label Studio configurations and guidelines\n",
    "├── ml/                  # All ML-related code\n",
    "│   ├── data/            # Data artifacts (managed by DVC)\n",
    "│   ├── models/          # Model definitions and architectures\n",
    "│   ├── notebooks/       # Exploratory notebooks\n",
    "│   ├── training/        # Training and evaluation scripts\n",
    "│   └── utils/           # Helper functions for ML tasks\n",
    "├── services/            # Application microservices\n",
    "│   ├── api/             # FastAPI backend service\n",
    "│   ├── worker/          # Prefect worker and flows\n",
    "│   └── agent-ui/        # UI for human escalation\n",
    "├── tests/               # Unit and integration tests\n",
    "├──.env                 # Environment variables (not committed to git)\n",
    "├──.gitignore           # Files and directories to ignore\n",
    "├── dvc.yaml             # DVC pipeline definition\n",
    "├── docker-compose.yml   # Local infrastructure stack\n",
    "├── environment.yml      # Conda environment for development\n",
    "└── requirements.txt     # Pip dependencies for production\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54d01d",
   "metadata": {},
   "source": [
    "### System Data Flow\n",
    "<img src=\"assets/system_data_flow.png\" alt=\"System Data Flow\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503c5d1",
   "metadata": {},
   "source": [
    "\n",
    "1.\tUser Chat UI → API Gateway\n",
    "    - The user types a message (like “Where’s my order?”).\n",
    "    - That message goes into the API Gateway (the entry door).\n",
    "\n",
    "2.\tAPI Gateway → Bot Manager\n",
    "    - The API Gateway hands the message to the Bot Manager.\n",
    "    - The Bot Manager is the brain of the system — it decides what to do with the message.\n",
    "\n",
    "3.\tBot Manager (Different Paths), Depending on the type of message, the Bot Manager has multiple options:\n",
    "    - Detect Intent: Understands what the user wants (e.g., track order, ask about a product, FAQs).\n",
    "    - Extract Information: Pulls key details like order ID, product name, or location.\n",
    "\n",
    "    - Search Knowledge (Vector + Keywords):\n",
    "        - Looks up answers from the Product Catalog or FAQs.\n",
    "        - Uses two techniques: keyword search (Elasticsearch) and meaning-based search (FAISS).\n",
    "\n",
    "    - Generate Answer (LLM + RAG):\n",
    "        - If the answer needs to be written out more naturally, it asks a language model.\n",
    "        - The model uses retrieved context so the answer is factual and not hallucinated.\n",
    "\n",
    "    - Human Agent (Fallback): If the system isn’t confident, it sends the chat to a real person.\n",
    "\n",
    "4.\tFinal Response → User Chat UI\n",
    "    - Whatever action is chosen (search, database lookup, LLM, or human agent), the Bot Manager puts together a final response.\n",
    "    - That response goes back through the API → and shows up in the user’s chat window.\n",
    "\n",
    "In short:\n",
    "```\n",
    "User asks → API receives → Bot Manager thinks → chooses (search, database, LLM, or human) → creates final answer → sends back to user.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfdade",
   "metadata": {},
   "source": [
    "### Training Pipeline\n",
    "<img src=\"assets/training_pipeline.png\" alt=\"Training Pipeline\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152582b2",
   "metadata": {},
   "source": [
    "\n",
    "1.\tCollect Data\n",
    "    - Gather raw data: chat logs, FAQs, product info, and past user interactions.\n",
    "2.\tClean & Prepare Data\n",
    "    - Remove noise, fix formatting, and organize the text so it’s ready for training.\n",
    "3.\tAdd Labels\n",
    "    - Tag the data with useful information:\n",
    "        - e.g., mark what the intent is (“track order”),\n",
    "        - highlight entities (like order_id or city).\n",
    "4.\tCreate Features\n",
    "    - Transform the raw text into a machine-readable format (numbers, vectors, embeddings).\n",
    "5.\tTrain Different Models\n",
    "    - Intent Model → Learns to recognize what the user wants.\n",
    "    - Entity Extractor → Learns to pick out important details (order ID, product name).\n",
    "    - Search Model → Learns to find the right info in the catalog or knowledge base.\n",
    "    - Answer Generator → Learns to generate natural, human-like responses.\n",
    "6.\tEvaluate Models\n",
    "    - Test each model to see how well it performs (accuracy, precision, recall).\n",
    "    - Only good models move forward.\n",
    "7.\tSave Models (MLflow Registry)\n",
    "    - Store the approved models in a central registry so they’re tracked and versioned.\n",
    "8.\tBuild Package (Docker Image)\n",
    "    - Bundle the model + code into a portable package (Docker).\n",
    "    - This makes it easy to run anywhere.\n",
    "9.\tDeploy to Cluster (K8s via Helm)\n",
    "    - Deploy the package to a Kubernetes cluster.\n",
    "    - Now the model is live and can serve real user queries.\n",
    "\n",
    "In short:\n",
    "```\n",
    "Data → Clean → Label → Features → Train → Evaluate → Save → Package → Deploy → Ready for Users.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ea541",
   "metadata": {},
   "source": [
    "## Automatic Pipeline Run Script\n",
    "\n",
    "### [`Docker Compose`](docker-compose.yml) :  Orchestrates services/containers.\n",
    "This docker compose file will run the following services automatically, with all configuration of every service set in .env for security:\n",
    "- MinIO\n",
    "- PostgreSQL\n",
    "- MLflow\n",
    "- Prefect\n",
    "- Grafana\n",
    "- Prometheus\n",
    "- Kafka\n",
    "- Elasticsearch\n",
    "\n",
    "**Quickstart :**\n",
    "\n",
    "1. To run docker compose :\n",
    "    ```bash\n",
    "    docker-compose up -d --build\n",
    "    ```\n",
    "\n",
    "2. To check the status of all running containers:\n",
    "    ```bash\n",
    "    docker-compose ps\n",
    "    ```\n",
    "\n",
    "3. To shut down the stack:\n",
    "    ```bash\n",
    "    docker-compose down\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36533901",
   "metadata": {},
   "source": [
    "### [`DVC`](dvc.yaml) :  Orchestrates data & ML pipeline. \n",
    "This DVC will run the following stages/script automatically.\n",
    "-  Step 1: download raw data.\n",
    "-  Step 2: preprocess/clean data.\n",
    "-  Step 3: feature engineering.\n",
    "-  Step 4: train model.\n",
    "-  Step 5: evaluate and push metrics.\n",
    "\n",
    "**QuickStart :**\n",
    "1. To run DVC :\n",
    "    ```bash\n",
    "    dvc repro\n",
    "    ```\n",
    "2. To check the status of DVC :\n",
    "    ```bash\n",
    "    dvc status\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8497691",
   "metadata": {},
   "source": [
    "### [PostgreSQL Initialization](`infra/postgres/init.sql`)\n",
    "\n",
    "Our docker compose file is need one more configuration, which is create database. here, we will create database `mlflow` in `postgres` service that being set to run automatically when the first time docker compose is running.\n",
    "\n",
    "### [MLflow Custom Dockerfile](`infra/mlflow/Dockerfile`)\n",
    "\n",
    "Why we need custom dockerfile for mlflow?, its because mlflow need to be set up manually everytime it start, and by containerizing all setup like dependencies, python etc. it just need to run the container and being set up automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627925f",
   "metadata": {},
   "source": [
    "### Local Service Endpoints and Access\n",
    "\n",
    "To make it easier to navigate and access all service server. we make reference that tell us all service endpoint and default credential.\n",
    "\n",
    "| Service           | Purpose                        | Local URL                    | Default User | Default Password |\n",
    "| ----------------- | ------------------------------ | ---------------------------- | ------------ | ---------------- |\n",
    "| **Chatbot API**   | Main application endpoint      | `http://localhost:8000/docs` | N/A          | N/A              |\n",
    "| **MLflow**        | Experiment Tracking & Registry | `http://localhost:5001`      | N/A          | N/A              |\n",
    "| **MinIO Console** | S3 Artifact Storage UI         | `http://localhost:9001`      | `minioadmin` | `minioadmin`     |\n",
    "| **Grafana**       | Monitoring Dashboards          | `http://localhost:3000`      | `admin`      | `admin`          |\n",
    "| **Prometheus**    | Metrics Server UI              | `http://localhost:9090`      | N/A          | N/A              |\n",
    "| **Prefect UI**    | Workflow Orchestration         | `http://localhost:4200`      | N/A          | N/A              |\n",
    "| **PostgreSQL**    | Database Connection            | `localhost:5432`             | `postgres`   | `postgres`       |\n",
    "| **Elasticsearch** | Search API                     | `http://localhost:9200`      | N/A          | N/A              |\n",
    "| **Kafka**         | Broker Connection              | `localhost:9092`             | N/A          | N/A              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c65ac",
   "metadata": {},
   "source": [
    "## Environment and Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bab6a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
